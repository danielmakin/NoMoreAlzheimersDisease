{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No More Alzheimer's Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Each necessary file is downloaded, unnecessary fields are removed and labels are assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/tqk_65w50d95gtyf68q0hv880000gn/T/ipykernel_14558/1745257678.py:6: DtypeWarning: Columns (20,105) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  labels = pd.read_csv('Data/ADNIMERGE_15Jun2023.csv')[['RID', 'VISCODE', 'DX']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''This function reads and does some pre-processing for the data files provided.'''\n",
    "\n",
    "ugot = pd.read_csv(\"Data/Plasma/UGOTPTAU181_06_18_20_27Nov2023.csv\")\n",
    "upenn = pd.read_csv(\"Data/Plasma/UPENNPLASMA_27Nov2023.csv\")\n",
    "# Read the labels that will be used for both of the files\n",
    "labels = pd.read_csv('Data/ADNIMERGE_15Jun2023.csv')[['RID', 'VISCODE', 'DX']]\n",
    "\n",
    "# Now merge these two files and remove any rows that have null values\n",
    "ugot = pd.merge(ugot, labels[['RID', 'VISCODE', 'DX']], on=['RID', 'VISCODE'], how='left').dropna(subset=['RID', 'PLASMAPTAU181', 'DX'])\n",
    "upenn = pd.merge(upenn, labels[['RID', 'VISCODE', 'DX']], on=['RID', 'VISCODE'], how='left').dropna()\n",
    "# Update Stamp is not relevant for this test\n",
    "ugot.drop(['update_stamp', 'VISCODE', 'VISCODE2', 'COMMENT', 'VID', 'EXAMDATE'], axis=1, inplace=True)\n",
    "# This puts the labels in the same format used for the tests\n",
    "ugot = ugot.replace(\"Dementia\", \"AD\").replace(\"CN\", \"SCD\").reset_index(drop=True)\n",
    "ugot.to_csv(\"Data/ProcessedData/UGOT.csv\", index=False)\n",
    "\n",
    "upenn.drop(['update_stamp', 'VISCODE'], axis=1, inplace=True)\n",
    "# This puts the labels in the same format used for the tests\n",
    "upenn = upenn.replace(\"Dementia\", \"AD\").replace(\"CN\", \"SCD\").reset_index(drop=True)\n",
    "upenn.to_csv(\"Data/ProcessedData/UPENN.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXy(df):\n",
    "    '''\n",
    "    Seperate the label from the feature data.\n",
    "    \n",
    "    Args : \n",
    "        df (DataFrame) : The data to be seperated\n",
    "    Returns :\n",
    "        X (DataFrame) : The feature data.\n",
    "        y (list) : The labels associated.'''\n",
    "    \n",
    "    # Independant\n",
    "    X = df.drop(df.columns[-1], axis=1).values\n",
    "    # Dependant\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_different_classes(df):\n",
    "    '''\n",
    "        This function returns the three different classes, seperated into different dataframes.\n",
    "        \n",
    "        Args :\n",
    "            df (DataFrame) : The dataframe containing all the label and feature data.\n",
    "        Returns :\n",
    "            SCD, MCI, AD : Each of the different labels (with the feature data) seperated into dataframes'''\n",
    "    \n",
    "    SCD = df.loc[df[\"DX\"] == \"SCD\"]\n",
    "    MCI = df.loc[df[\"DX\"] == \"MCI\"]\n",
    "    AD = df.loc[df[\"DX\"] == \"AD\"]\n",
    "\n",
    "    return SCD, MCI, AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_data(SCD, MCI, AD, TestingFactor = 0.25):\n",
    "    '''\n",
    "        Now we seperate the training and testing data. Currently using a 25% training test split.\n",
    "\n",
    "        Args : \n",
    "            SCD, MCI, AD (DataFrame) : Data to be split, already split into classes.\n",
    "            TestingFactor (int) [OPTIONAL] : Amount of data put in testing.\n",
    "        Returns :\n",
    "            SCD, MCI, AD, TestData (DataFrame) : Each of the sections of data.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    SCD, TempSCD = train_test_split(SCD, test_size=TestingFactor)\n",
    "    MCI, TempMCI = train_test_split(MCI, test_size=TestingFactor)\n",
    "    AD, TempAD = train_test_split(AD, test_size=TestingFactor)\n",
    "\n",
    "    # concatenate the lists\n",
    "    TempData = [TempSCD, TempMCI, TempAD]\n",
    "    TestData = pd.concat(TempData)\n",
    "\n",
    "    # return required info\n",
    "    return SCD, MCI, AD, TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_svm(df1, df2):\n",
    "    '''\n",
    "        Constructs an SVM with the datasets provided.\n",
    "\n",
    "        Args :\n",
    "            Datasets (DataFrame) : Two DataFrames that the SVM must be made upon.\n",
    "        Returns :\n",
    "            Classifier (SVC) : This is the margin that the data must be acted upon.\n",
    "            Scaler (StandardScaler) : Used in testing.\n",
    "    '''\n",
    "\n",
    "    # First the two dataframes should be combined\n",
    "    df = [df1, df2]\n",
    "    df = pd.concat(df)\n",
    "    # Get data in the format required\n",
    "    X, y = getXy(df)\n",
    "    # Train the classifier\n",
    "    sc = StandardScaler()\n",
    "    X = sc.fit_transform(X)\n",
    "    # Fit to the classifier\n",
    "    classifier = SVC(kernel='rbf', random_state=0)\n",
    "    classifier.fit(X, y)\n",
    "\n",
    "    return classifier, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(classifier, scaler, X):\n",
    "    '''\n",
    "        Test the dataset with each individual SVM\n",
    "\n",
    "        Args :\n",
    "            classifier (SVM) : The Support Vector Machine used for this test\n",
    "            scaler (StandardScaler) : This allows the test data to be scaled to the same proportions as the test data\n",
    "            X (DataFrame) : The feature data WITHOUT labels\n",
    "        Returns :\n",
    "            y_pred (list) : the predicted y-value for each item\n",
    "    '''\n",
    "    # now perform the classification\n",
    "    X = scaler.fit_transform(X)\n",
    "    y_pred = classifier.predict(X)\n",
    "    # return the result of this transaction\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Decision Tree SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bdt(file_path):\n",
    "    '''\n",
    "        Perform the Binary Decision Tree SVM classification method.\n",
    "        \n",
    "        Args :\n",
    "            file_path (String) : Path to the data\n",
    "        Returns :\n",
    "            time (float) : Time taken to perform the test.\n",
    "            accuracy (float) : The accuracy of the model.\n",
    "            confusion_matrix : Confusion matrix for the accuracy.'''\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    start = timer()\n",
    "    # Now seperate the labels\n",
    "    SCD, MCI, AD = get_different_classes(df)\n",
    "    # get the test data required, leave training data\n",
    "    SCD, MCI, AD, TestData = split_test_data(SCD, MCI, AD)\n",
    "    \n",
    "\n",
    "    # seperate the labels and the data features\n",
    "    X_test, y_test = getXy(TestData)\n",
    "\n",
    "    # Concatenate MCI\n",
    "    MCIoAD = [MCI, AD]\n",
    "    MCIoAD = pd.concat(MCIoAD)\n",
    "    # Map to the same class\n",
    "    MCIoAD = MCIoAD.replace(\"MCI\", \"MCIoAD\").replace(\"AD\", \"MCIoAD\")\n",
    "\n",
    "    # Test to perform the first step of the DT\n",
    "    SCDMCIAD, scalersma = construct_svm(SCD, MCIoAD)\n",
    "    # SVM for the second level of the BDT\n",
    "    MCIAD, scalarma = construct_svm(MCI, AD)\n",
    "\n",
    "    # Perform the test for the first level of BDT\n",
    "    SCDoMCIAD = test(SCDMCIAD, scalersma, X_test)\n",
    "\n",
    "    # Now run entire set through the MCI, AD classifier. \n",
    "    # However, only the non-SCD items in the previous test will be used\n",
    "    MCIoAD = test(MCIAD, scalarma, X_test)\n",
    "\n",
    "    # Now colalate the results together taking the two classifiers into account\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(SCDoMCIAD)):\n",
    "        if SCDoMCIAD[i] == 'SCD':\n",
    "            results.append('SCD')\n",
    "        else:\n",
    "            # Adds the result of the second level\n",
    "            results.append(MCIoAD[i])\n",
    "\n",
    "    # Print out the time taken and results\n",
    "    end = timer()\n",
    "\n",
    "    # construct a confusion matrix\n",
    "    cm = confusion_matrix(y_test, results)\n",
    "    # print(cm)\n",
    "    # print(\"Accuracy : \" + str(accuracy_score(y_test, results)))\n",
    "\n",
    "    return end-start, accuracy_score(y_test, results), cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnevOne SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_v_one_classifier(file_path):\n",
    "    '''\n",
    "        Perform the One v One SVM classification method.\n",
    "        \n",
    "        Args :\n",
    "            file_path (String) : Path to the data\n",
    "        Returns :\n",
    "            time (float) : Time taken to perform the test.\n",
    "            accuracy (float) : The accuracy of the model.\n",
    "            confusion_matrix : Confusion matrix for the accuracy.'''\n",
    "    \n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Keep track of the time taken\n",
    "    start = timer()\n",
    "    # Now seperate the labels\n",
    "    SCD, MCI, AD = get_different_classes(df)\n",
    "    # get the test data required, leave training data\n",
    "    SCD, MCI, AD, TestData = split_test_data(SCD, MCI, AD)\n",
    "\n",
    "    # seperate the labels and the data features\n",
    "    X_test, y_test = getXy(TestData)\n",
    "\n",
    "    # Construct the three different SVMs\n",
    "    SCDMCI, scalersm = construct_svm(SCD, MCI)\n",
    "    MCIAD, scalerma = construct_svm(MCI, AD)\n",
    "    SCDAD, scalersa = construct_svm(SCD, AD)\n",
    "\n",
    "    # Perform the individual tests\n",
    "    pred1 = test(SCDMCI, scalersm, X_test)\n",
    "    pred2 = test(MCIAD, scalerma, X_test)\n",
    "    pred3 = test(SCDAD, scalersa, X_test)\n",
    "\n",
    "    # This now gets the most common item to appear in the lists\n",
    "\n",
    "    common = [\"-1\" for i in range(len(pred1))]\n",
    "    notClassified = 0\n",
    "\n",
    "    # classify values that have atleast a 2 in a majority voting scheme\n",
    "    for i in range(len(pred1)):\n",
    "        if (pred1[i] == pred2[i]) or (pred1[i] == pred3[i]):\n",
    "            common[i] = pred1[i]\n",
    "        elif (pred2[i] == pred3[i]):\n",
    "            common[i] = pred2[i]\n",
    "\n",
    "    cm = confusion_matrix(y_test, common)\n",
    "\n",
    "    end = timer()\n",
    "\n",
    "    return end-start, accuracy_score(y_test, common), cm \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4262023217247098\n"
     ]
    }
   ],
   "source": [
    "bdt_up_time, bdt_up_acc, bdt_up_cm = bdt('Data/ProcessedData/UPENN.csv')\n",
    "bdt_ug_time, bdt_ug_acc, bdt_ug_cm = bdt('Data/ProcessedData/UGOT.csv')\n",
    "\n",
    "ovo_up_time, ovo_up_acc, ovo_up_cm = one_v_one_classifier('Data/ProcessedData/UPENN.csv')\n",
    "ovo_ug_time, ovo_ug_acc, ovo_ug_cm = one_v_one_classifier('Data/ProcessedData/UGOT.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
